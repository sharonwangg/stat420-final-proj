---
title: "How do we best predict the total confirmed COVID-19 cases count?"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r library_setup, message=FALSE, include=FALSE}
libraries <- c("tidyverse", "faraway", "stringr", "gtools", "readr")
lapply(libraries, require, character.only = TRUE)
```

## Introduction
We would like to explore which variables can best predict the total confirmed COVID-19 count of a region. This dataset includes quantitative and qualitative data about the COVID-19 pandemic. The data gathered at https://github.com/open-covid-19/data is collected from many different sources, including the University of Oxford, the NOAA, the WorldBank Database, WorldPop, DataCommons, and many more. The full list can be found in the README file. Some variables of interest include `cancel_public_events`, `school_closing`, and `mobility_transit_stations` This is interesting to us because we would like to discover exactly what predictors can be predict the total confirmed COVID-19 count of a region to see what really affects the impact of the virus, and we personally believe that variables such as population density, restrictions on gatherings, population age, cormorbidity mortality rate, and general mobility affect the impact of COVID-19 on a region the most. The goal of the model is to see which variables truly affect the COVID-19 count of a region. 

## Methods
### Data Preparation
```{r echo=TRUE, message=FALSE, warning=FALSE}
# github repo: https://github.com/open-covid-19/data
# loading this data takes awhile, storing as .csv might be faster although out of date
# data_full <- read_csv("https://storage.googleapis.com/covid19-open-data/v2/main.csv")
# write_csv(data_full, "data_full.csv")
data_full <- read_csv("data_full.csv", guess_max = 3825711)
```

Figuring out which columns have a lot of NAs and avoiding them
- although issue might be in certain countries and earlier dates (before the pandemic started)
```{r include=FALSE}
apply(data_full, MARGIN = 2, FUN = function(x) { length(unique(x)) })
```

Take complete cases so linear regression doesn't fail. Use only certain columns we need for checking
``` {r}
# include country to be able to filter by US
# subregion2_code = FIPS (zip) code
must_include = c("total_confirmed", "country_code")

possible = c("population", "urban_population", "population_density", "human_development_index", "gdp_per_capita", "date", "life_expectancy", "comorbidity_mortality_rate", "physicians", "school_closing", "cancel_public_events", "restrictions_on_gatherings", "stay_at_home_requirements", "mobility_retail_and_recreation", "mobility_parks", "mobility_transit_stations", "mobility_workplaces", "mobility_residential")

possible_list <- data.frame()
for (i in 1:length(possible)) {
  current = c(must_include, possible[i])
  complete_vec = complete.cases(data_full[, current])
  clean_data = data_full[complete_vec, ]
  good = nrow(clean_data) > 1
  add = 1
  while (good && i + add < length(possible)) {
    possible_list = rbind(possible_list, data.frame(columns = str_c(current, collapse = ","), rows = nrow(clean_data), cols = length(current)))
    current = c(current, possible[i + add])
    complete_vec = complete.cases(data_full[, current])
    clean_data = data_full[complete_vec, ]
    good = nrow(clean_data) > 1
    add = add + 1
  }
}

max_cols = possible_list[which.max(possible_list$cols),]
columns_kept = str_split(max_cols$columns, ",")[[1]]
(rows_excluded = possible[!(possible %in% columns_kept)])
max_cols$rows
```

```{r}
complete_vec = complete.cases(data_full[, columns_kept])
clean_data = data_full[complete_vec, columns_kept]
```

Normalizing the population counts because absolute numbers probably don't add any predictive power
```{r}
population_data = subset(clean_data, select = grepl("population_age_", names(clean_data)))
total_populations = apply(population_data, MARGIN = 1, FUN = sum)
normalized_population = data.frame(t(t(population_data) / total_populations))
```

Setting the `date` column as a Date object, rather than a factor variable
```{r}
clean_data$date <- as.Date(clean_data$date)
```

Select columns that would best fit our goal
```{r}
clean_data_subset = clean_data %>%
  bind_cols(normalized_population) %>%
  subset(country_code == "US") %>%
  subset(select = -c(country_code))
```

Then, remove some variables who had VIFs as large as the thousands and tens of thousands, which would negatively impact the overall model.
```{r}
# saving the cleaned dataset so that we don't have to keep running above code
write_csv(clean_data_subset, "clean_data_subset.csv")
clean_data_subset = read.csv("clean_data_subset.csv")
clean_data_subset = clean_data_subset %>% subset(select = c(total_confirmed, school_closing, cancel_public_events, stay_at_home_requirements, mobility_parks, mobility_transit_stations))
names(clean_data_subset)
```
Above are the variables that seem to initially have a strong connection to the response, `total_confirmed`.

```{r}
clean_data_subset$date = NULL
```

### Scatter Plot Matrix
```{r fig.height=20, fig.width=20}
pairs(clean_data_subset, pch = 20, cex = 3)
```

```{r warning=FALSE}
cor(clean_data_subset)
```
- `school_closing` and `cancel_public_events` and `stay_at_home_requiremenets` have collinearity.
- `cancel_public_events` and `stay_at_home_requirements` have collinearity.

Perform backwards and forward AIC and BIC on the full additive model, and then use anova() to pick the best model out of the 4. 
```{r}
# additive
full_add_model = lm(total_confirmed ~ ., data = clean_data_subset)
```

```{r}
# back aic
back_aic = step(full_add_model, direction = "backward", trace = 0)
# back bic
n = length(full_add_model$residuals)
back_bic = step(full_add_model, direction = "backward", k = log(n), trace = 0)
# forward aic
null_model = lm(total_confirmed ~ 1, data = clean_data_subset)
forward_aic = step(
  null_model,
  scope = total_confirmed ~ school_closing + cancel_public_events + stay_at_home_requirements + mobility_parks + mobility_transit_stations,
  direction = "forward", trace = 0)
# forward bic
forward_bic = step(
  null_model, 
  scope = total_confirmed ~ school_closing + cancel_public_events + stay_at_home_requirements + mobility_parks + mobility_transit_stations, 
  direction = "forward", k = log(n), trace = 0)
```

```{r}
# use anova to compare to models created from AIC and BIC
(back_aic_pvalue = anova(back_aic, full_add_model)[2, "Pr(>F)"])
(back_bic_pvalue = anova(back_bic, full_add_model)[2, "Pr(>F)"])
(forward_aic_pvalue = anova(forward_aic, full_add_model)[2, "Pr(>F)"])
(forward_bic_pvalue = anova(forward_bic, full_add_model)[2, "Pr(>F)"])
```
- The model created from backwards AIC predicts better than the interaction model with a p-value of `r back_aic_pvalue`. 
- The model created from backwards BIC predicts better than the interaction model with a p-value of `r back_bic_pvalue`. 
- The model created from forwards AIC predicts better than the interaction model with a p-value of `r forward_aic_pvalue`.
- The model created from forwards BIC predicts better than the interaction model with a p-value of `r forward_bic_pvalue`.
- The model with the highest p-value are the backward AIC model and the forward AIC model.

```{r}
back_aic$coef
forward_aic$coef 
```
The coefficients of these two models are the same, so we can use either.

```{r}
step_from_add = back_aic
```

```{r}
vif(step_from_add)
```

```{r}
plot(step_from_add$model)
```

The vifs are not terrible on this model, however, based on these plots, it is possible to do better.

```{r message=FALSE, warning=FALSE}
#boxcox model
library(MASS)
library(faraway)
boxcox(step_from_add, plotit = TRUE)
```

Based on the boxcox plot, it seems that we should be using a logarithmic transformation so we start by taking the log of the response variable.

```{r}
log_step_from_add = lm(log(total_confirmed) ~ stay_at_home_requirements + mobility_parks + mobility_transit_stations, data = clean_data_subset)
```
This will likely be one of our final models.

Next, perform backwards and forward AIC and BIC on the interactive model that goes up to 2-way interactions, and then use anova() to pick the best model out of the 4. 
```{r}
# interaction
int_model = lm(total_confirmed ~ (school_closing + cancel_public_events + stay_at_home_requirements + mobility_parks + mobility_transit_stations)^2, data = clean_data_subset)
```

Using our model that includes all the interactions, we can use those as starting points for the AIC and BIC created models. 
```{r}
# back aic
back_aic = step(int_model, direction = "backward", trace = 0)
# back bic
n = length(int_model$residuals)
back_bic = step(int_model, direction = "backward", k = log(n), trace = 0)
# forward aic
null_model = lm(total_confirmed ~ 1, data = clean_data_subset)
forward_aic = step(
  null_model,
  scope = total_confirmed ~ (school_closing + cancel_public_events + stay_at_home_requirements + mobility_parks + mobility_transit_stations)^2,
  direction = "forward", trace = 0)
# forward bic
forward_bic = step(
  null_model, 
  scope = total_confirmed ~ (school_closing + cancel_public_events + stay_at_home_requirements + mobility_parks + mobility_transit_stations)^2, 
  direction = "forward", k = log(n), trace = 0)
```

```{r}
# use anova to compare to models created from AIC and BIC
(back_aic_pvalue = anova(back_aic, int_model)[2, "Pr(>F)"])
(back_bic_pvalue = anova(back_bic, int_model)[2, "Pr(>F)"])
(forward_aic_pvalue = anova(forward_aic, int_model)[2, "Pr(>F)"])
(forward_bic_pvalue = anova(forward_bic, int_model)[2, "Pr(>F)"])
```
- The model created from backwards AIC predicts better than the interaction model with a p-value of `r back_aic_pvalue`. 
- The model created from backwards BIC predicts better than the interaction model with a p-value of `r back_bic_pvalue`. 
- The model created from forwards AIC predicts better than the interaction model with a p-value of `r forward_aic_pvalue`.
- The model created from forwards BIC predicts better than the interaction model with a p-value of `r forward_bic_pvalue`.

From the above p-values, it seems that the model created from backwards AIC and the model created from forwards AIC are the best, so we will compare those.
```{r}
back_aic$coef
forward_aic$coef
```
Since they have the same coefficients, these 2 models are the same.

```{r}
step_from_int = back_aic
vif(step_from_int)
```

```{r}
plot(step_from_int$model)
```

The vifs are not terrible on this model, however, based on these plots, it is possible to do better.

```{r message=FALSE, warning=FALSE}
#boxcox model
library(MASS)
library(faraway)
boxcox(step_from_int, plotit = TRUE)
```

Based on the boxcox plot, it seems that we should be using a logarithmic transformation so we start by taking the log of the response variable.
```{r}
log_step_from_int = lm(log(total_confirmed) ~ stay_at_home_requirements + mobility_parks + mobility_transit_stations + stay_at_home_requirements:mobility_parks + mobility_parks:mobility_transit_stations, data = clean_data_subset)
```
This will likely be one of our final models.

## Results
### Check Assumptions
#### Convenience Functions
```{r message=FALSE, warning=FALSE}
fitted_vs_residuals = function(model, title) {
  plot(model$fitted.values, model$residuals, pch = 20,
       xlab = "Fitted", ylab = "Residuals", main = title)
  abline(h = 0, lwd = 2, col = "red")
}

library(lmtest)
get_bp_pvalue = function(model) {
  as.vector(bptest(model)$p.value)
}

residual_histogram = function(model, title) {
  hist(model$residuals,
       xlab = "Residuals", main = title, breaks = 50)
}

qq_plot = function(model, title) {
  qqnorm(model$residuals, main = title, pch = 20)
  qqline(model$residuals, lwd = 2, col = "red")
}

get_shapiro_pvalue = function(model) {
  shapiro.test(model$residuals)$p.value
}
```

#### Fitted vs. Residuals Plots
```{r}
par(mfrow = c(1, 2))
fitted_vs_residuals(log_step_from_add, "Fitted vs. Residuals Plot for Step from Add (Log)")
fitted_vs_residuals(log_step_from_int, "Fitted vs. Residuals Plot for Step from Int (Log)")
```
#### Histogram of Residuals
```{r}
par(mfrow = c(1, 2))
residual_histogram(log_step_from_add, "Residual Histogram for Step from Add (Log)")
residual_histogram(log_step_from_int, "Residual Histogram for Step from Int (Log)")
```

#### Normal Q-Q Plots
```{r}
par(mfrow = c(1, 2))
qq_plot(log_step_from_add, "QQ Plot for Step from Add (Log)")
qq_plot(log_step_from_int, "QQ Plot for Step from Int (Log)")
```

#### Compare Breusch-Pagan and Shapiro-Wilkes p-values
```{r}
pvalue_data = data.frame(Models = c("Step from Add (Log)", "Step from Int (Log)"),
                         BP.pvalue = c(get_bp_pvalue(log_step_from_add), get_bp_pvalue(log_step_from_int)),
                         Shapiro.pvalue = c(get_shapiro_pvalue(log_step_from_add), get_shapiro_pvalue(log_step_from_int)))

library(knitr)
library(kableExtra)
kable(pvalue_data) %>% kable_styling(bootstrap_options = c("striped", "hover", "responsive", full_width = FALSE, position = "left"))
```

#### Check model statistics for the best model of the 4 models created from using step on the simple additive model with a logged response.
- LOOCV
```{r}
calc_loocv_rmse = function(model) {
  sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))
}

calc_loocv_rmse(log_step_from_add)
```

- R^2
```{r}
summary(log_step_from_add)$r.squared
summary(log_step_from_add)$adj.r.squared
```
These look good, the loocv-RMSE is low and the adjusted R^2 is high.

#### Check model statistics for the best model of the 4 models created from using step on the interactive model with a logged response.
- LOOCV
```{r}
calc_loocv_rmse(log_step_from_int)
```

- R^2
```{r}
summary(log_step_from_int)$r.squared
summary(log_step_from_int)$adj.r.squared
```

To improve the best model of the 4 models created from using step on the simple additive model with a logged response, try some transformations on the predictors.
```{r}
log_step_from_add$coef

transformed_log_step_from_add = lm(log(total_confirmed) ~ stay_at_home_requirements + log(mobility_parks + abs(min(mobility_parks)) + 1) + log(mobility_transit_stations + abs(min(mobility_transit_stations)) + 1), data = clean_data_subset)

summary(transformed_log_step_from_add)
plot(transformed_log_step_from_add$model)
plot(transformed_log_step_from_add)
```

#### Outlier Diagnostics
- Cook's distance + influence

```{r}
final_model = transformed_log_step_from_add

cds = cooks.distance(final_model)
n = length(final_model$fitted.values)
influential_cd = 4 / n
influential_points = cds > influential_cd

clean_without_influence = clean_data_subset[!influential_points,]

refit_final_model = lm(final_model$call$formula, data = clean_without_influence)
null_log_model = lm(log(total_confirmed) ~ 1, data = clean_without_influence)
step_refit_final_model = step(
  null_log_model,
  scope = final_model$call$formula,
  direction = "forward", trace = 0)

# shapiro.test(step_refit_final_model$residuals)$p.value
as.vector(bptest(step_refit_final_model)$p.value)

plot(step_refit_final_model$model)
plot(step_refit_final_model)
```
