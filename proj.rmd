---
title: "Data Cleaning"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r library_setup, message=FALSE}
libraries <- c("tidyverse", "faraway", "stringr", "gtools", "readr")
lapply(libraries, require, character.only = TRUE)
```

## Introduction
We would like to explore which variables can best predict the total confirmed COVID-19 count of a region. This dataset includes quantitative and qualitative data about the COVID-19 pandemic. The data gathered at https://github.com/open-covid-19/data is collected from many different sources, including the University of Oxford, the NOAA, the WorldBank Database, WorldPop, DataCommons, and many more. The full list can be found in the README file. Some variables of interest include population_density, restrictions_on_gatherings, and mobility_retail_and_recreation. This is interesting to us because we would like to discover exactly what predictors can be predict the total confirmed COVID-19 count of a region to see what really affects the impact of the virus, and we personally believe that variables such as population density, restrictions on gatherings, population age, cormorbidity mortality rate, and general mobility affect the impact of COVID-19 on a region the most. The goal of the model is to see which variables truly affect the COVID-19 count of a region. 

## Methods
### Data Preparation
```{r}
# github repo: https://github.com/open-covid-19/data
# loading this data takes awhile, storing as .csv might be faster although out of date
#data_full <- read_csv("https://storage.googleapis.com/covid19-open-data/v2/main.csv")
# write_csv(data_full, "data_full.csv")
data_full <- read_csv("data_full.csv", guess_max = 3825711)
```

Figuring out which columns have a lot of NAs and avoiding them
- although issue might be in certain countries and earlier dates (before the pandemic started)
```{r}
unique(data_full$locality_name)
```

```{r}
apply(data_full, MARGIN = 2, FUN = function(x) { length(unique(x)) })
```

Take complete cases so linear regression doesn't fail
- use only certain columns we need for checking
``` {r}
# include country to be able to filter by US
# subregion2_code = FIPS (zip) code
must_include = c("total_confirmed", "country_code")

possible = c("population", "urban_population", "population_density", "human_development_index", "gdp_per_capita", "date", "life_expectancy", "comorbidity_mortality_rate", "physicians", "school_closing", "cancel_public_events", "restrictions_on_gatherings", "stay_at_home_requirements", "mobility_retail_and_recreation", "mobility_parks", "mobility_transit_stations", "mobility_workplaces", "mobility_residential")

possible_list <- data.frame()
for (i in 1:length(possible)) {
  current = c(must_include, possible[i])
  complete_vec = complete.cases(data_full[, current])
  clean_data = data_full[complete_vec, ]
  good = nrow(clean_data) > 1
  add = 1
  while (good && i + add < length(possible)) {
    possible_list = rbind(possible_list, data.frame(columns = str_c(current, collapse = ","), rows = nrow(clean_data), cols = length(current)))
    current = c(current, possible[i + add])
    complete_vec = complete.cases(data_full[, current])
    clean_data = data_full[complete_vec, ]
    good = nrow(clean_data) > 1
    add = add + 1
  }
}

max_cols = possible_list[which.max(possible_list$cols),]
columns_kept = str_split(max_cols$columns, ",")[[1]]
(rows_excluded = possible[!(possible %in% columns_kept)])
max_cols$rows
```

```{r}
complete_vec = complete.cases(data_full[, columns_kept])
clean_data = data_full[complete_vec, columns_kept]
clean_data$total_confirmed[1:100]
```

Normalizing the population counts because absolute numbers probably don't add any predictive power
```{r}
population_data = subset(clean_data, select = grepl("population_age_", names(clean_data)))
total_populations = apply(population_data, MARGIN = 1, FUN = sum)
normalized_population = data.frame(t(t(population_data) / total_populations))
```

Setting the `date` column as a Date object, rather than a factor variable
```{r}
clean_data$date <- as.Date(clean_data$date)
```

Select columns that would best fit our goal
```{r}
clean_data_subset = clean_data %>%
  bind_cols(normalized_population) %>%
  filter(country_code == "US") %>%
  subset(select = -country_code)
head(clean_data_subset)
```

```{r}
# saving the cleaned dataset so that we don't have to keep running above code
write_csv(clean_data_subset, "clean_data_subset.csv")
clean_data_subset = read.csv("clean_data_subset.csv")
clean_data_subset = clean_data_subset %>% subset(select = c(total_confirmed, school_closing, cancel_public_events, stay_at_home_requirements, mobility_parks, mobility_transit_stations))
names(clean_data_subset)
```

### Models
- Additive
- Interactions
- Polynomial
- Transformations (log, semi-log)
- step
  - backward
  - forward
  - both
  - BIC vs. AIC
- use of ANOVA to compare null vs. added model

```{r}
str(clean_data_subset)
clean_data_subset$date = NULL
```
Above are the variables that seem to initially have a strong connection to the response, `total_confirmed`.

### Correlation Analysis

### Scatter Plot Matrix
```{r fig.height=20, fig.width=20}
pairs(clean_data_subset, pch = 20)
```
```{r warning=FALSE}
cor(clean_data_subset)
```

```{r}
# additive
full_add_model = lm(total_confirmed ~ ., data = clean_data_subset)
summary(full_add_model)
```

```{r}
# interaction
int_model = lm(total_confirmed ~ . ^ 2  - school_closing:stay_at_home_requirements - cancel_public_events:stay_at_home_requirements, data = clean_data_subset)
summary(int_model)
```

```{r}
# back aic
back_aic = step(int_model, direction = "backward", trace = 0)
# back bic
n = length(int_model$residuals)
back_bic = step(int_model, direction = "backward", k = log(n), trace = 0)
# forward aic
null_model = lm(total_confirmed ~ 1, data = clean_data_subset)
forward_aic = step(
  null_model,
  scope = total_confirmed ~ (school_closing + cancel_public_events + stay_at_home_requirements + mobility_parks + mobility_transit_stations)^2,
  direction = "forward", trace = 0)
# forward bic
forward_bic = step(
  null_model, 
  scope = total_confirmed ~ (school_closing + cancel_public_events + stay_at_home_requirements + mobility_parks + mobility_transit_stations)^2, 
  direction = "forward", k = log(n), trace = 0)
summary(back_aic)
summary(back_bic)
```

```{r}
#boxcox model
boxcox(int_model, plotit = TRUE)
boxcox(int_model, plotit = TRUE, lambda = seq(-1.0, 1.0, by = 0.1))
```

```{r}
#log model
int_log_model = lm(log(total_confirmed) ~ . ^ 2 - school_closing:stay_at_home_requirements - cancel_public_events:stay_at_home_requirements, data = clean_data_subset)

back_aic_log_model = step(int_log_model, direction = "backward", trace = 0)
back_bic_log_model = step(int_log_model, direction = "backward", k = log(n), trace = 0)

null_model = lm(log(total_confirmed) ~ 1, data = clean_data_subset)

#forward aic log
forward_aic_log = step(
  null_model,
  scope = total_confirmed ~ (school_closing + cancel_public_events + stay_at_home_requirements + mobility_parks + mobility_transit_stations)^2,
  direction = "forward", trace = 0)

# forward bic log
forward_bic_log = step(
  null_model, 
  scope = total_confirmed ~ (school_closing + cancel_public_events + stay_at_home_requirements + mobility_parks + mobility_transit_stations)^2, 
  direction = "forward", k = log(n), trace = 0)

boxcox(back_aic_log_model, plotit = TRUE)
boxcox(back_bic_log_model, plotit = TRUE)
boxcox(forward_aic_log, plotit = TRUE)
boxcox(forward_bic_log, plotit = TRUE)
```

```{r}
(anova(back_aic_log_model, int_log_model)[2, "Pr(>F)"])
(anova(back_bic_log_model, int_log_model)[2, "Pr(>F)"])
(anova(forward_aic_log, int_log_model)[2, "Pr(>F)"])
(anova(forward_bic_log, int_log_model)[2, "Pr(>F)"])

(anova(back_aic_log_model, back_bic_log_model)[2, "Pr(>F)"])
length(coef(back_aic_log_model))
length(coef(back_bic_log_model))
```

```{r}
all.equal(back_aic$coef, back_bic$coef)
```
The model created from backwards AIC and backwards BIC are the same model.

```{r}
back_model = back_aic
```

```{r}
# use anova to compare to models created from AIC and BIC
(back_pvalue = anova(back_model, int_model)[2, "Pr(>F)"])
(forward_aic_pvalue = anova(forward_aic, int_model)[2, "Pr(>F)"])
(forward_bic_pvalue = anova(forward_bic, int_model)[2, "Pr(>F)"])
```
- The model created from backwards AIC/BIC predicts better than the interaction model with a p-value of `r back_aic_pvalue`. 
- The interaction model predicts better than the model created from forward AIC with a p-value of `r forward_aic_pvalue`.
- The interaction model predicts better than the model created from forward BIC with a p-value of `r forward_bic_pvalue`.
- Therefore, the model created from backwards AIC/BIC predicts better than the interaction model, which predicts better than the model created from forward AIC and forward BIC.

```{r}
best_model = back_bic_log_model
#coef(best_model)
car::vif(best_model)
plot(best_model$model)
```

### Assumption Checking
Linear model?
- Linear
- Independent
  - no/little collinearity --> variance inflation factor (VIF)
- Normal
  - Shapiro-Wilk test
  - Q-Q plot
- Equal variance
  - bptest

#### Residual Diagnostics
- fitted vs residuals plot

```{r}
plot(best_model$fitted.values, best_model$residuals, pch = 20,
    xlab = "Fitted", ylab = "Residuals", main = "Data from the Best Model")
abline(h = 0, lwd = 2, col = "red")
```

#### Check Constant Variance assumption
```{r}
# Breusch-Pagan Test
library(lmtest)
as.vector(bptest(best_model)$p.value)
```

#### Check Normality assumption
```{r}
# Histogram of Residuals
hist(best_model$residuals, 
     xlab = "Residuals", main = "Histogram of Residuals of Best Model", 
     breaks = 50)

# QQ Plot
qqnorm(best_model$residuals, main = "Normal Q-Q Plot of Best Model", pch = 20)
qqline(best_model$residuals, lwd = 2, col = "red")

# Shapiro-Wilkes Test
shapiro.test(best_model$residuals)$p.value
```

#### Outlier Diagnostics
- Cook's distance + influence

```{r}
cds = cooks.distance(best_model)
n = length(best_model$fitted.values)
influential_cd = 4 / n
influential_points = cds > influential_cd

clean_without_influence = clean_data_subset[!influential_points,]


clean_int_log_model = lm(log(total_confirmed) ~ . ^ 2 - school_closing:stay_at_home_requirements - cancel_public_events:stay_at_home_requirements, data = clean_without_influence)
new_best_model = step(clean_int_log_model, direction = "backward", k = log(nrow(clean_without_influence)), trace = 0)

shapiro.test(new_best_model$residuals)$p.value
as.vector(bptest(new_best_model)$p.value)

plot(new_best_model)
```

#### Model Statistics
- LOOCV (multiple linear regression)
```{r}
calc_loocv_rmse = function(model) {
  sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))
}

calc_loocv_rmse(new_best_model)
```

- R^2, RMSE (multiple linear regression)
```{r}
summary(new_best_model)$r.squared
summary(new_best_model)$adj.r.squared
summary(new_best_model)$coefficients
```